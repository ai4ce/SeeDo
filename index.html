<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SeeDo: Human Demo Video to Robot Action Plan via Vision Language Model">
  <meta name="keywords" content="Multimodal Large Language Model, Vision Language Model, Learning from Human Demonstrations">     
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VLM See, Robot Do</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> -->
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/copy2clipboard.js"></script>
  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  
  <script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script>
  <link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css">
</head>


<body>

<!-- Navigation bar. -->
<nav class="navbar is-light" role="navigation" aria-label="main navigation">
  <div class="container is-max-desktop">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://ai4ce.github.io/" target="_blank" rel="noopener noreferrer">
        <img src="./static/images/ai4ce_new_linear_notext.svg" alt="AI4CE Lab" style="height: 2.0rem;">
      </a>
      <a role="button" onclick="this.classList.toggle('is-active');document.querySelector('#'+this.dataset.target).classList.toggle('is-active');" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div id="navbarBasicExample" class="navbar-menu">
      <div class="navbar-start">
        <a class="navbar-item" href="https://www.nyu.edu/" target="_blank">
          <img src="./static/images/NYU_Long_RGB_Color.png" alt="NYU Logo" style="height: 2.0rem;">
        </a>
      </div>
    </div>
  </div>
</nav>

<!-- Title and authors. -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            VLM See, Robot Do:
          </h1>
          <h1 class="title is-2 publication-title">Human Demo Video to Robot Action Plan via Vision Language Model</h1>
          <!-- <div class="column is-full_width">
            <h2 class="title is-4"><a href="https://2025.ieee-icra.org">ICRA 2025 (Under Review)</a></h2>
          </div> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=ACFrFnYAAAAJ&hl=en">Beichen Wang</a><sup>*</sup><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://juexzz.github.io/">Juexiao Zhang</a><sup>*</sup><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/shuwen-dong-1342832b1">Shuwen Dong</a><sup>&dagger;</sup><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://irvingf7.github.io/">Irving Fang</a><sup>&dagger;</sup><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ">Chen Feng</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> New York University, </span>
            <br>
            <span class="author-block"><sup>*</sup> Equal contribution, first authors. <sup>&dagger;</sup> Equal contribution, second authors.</span>
          </div>



          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                 <!-- add here later. -->
                <a href=""                
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- <span class="link-block"> -->
                <!-- add here later. -->
               <!-- <a href=""                    
                  class="external-link button is-normal is-rounded is-dark">
                 <span class="icon">
                     <i class="fas fa-camera"></i>
                 </span>
                 <span>Appendix</span>
               </a> -->
             <!-- </span> -->
              <!-- Video Link. -->
              <span class="link-block">
                <!-- <a href="https://youtu.be/thC0PeAQxe0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a> -->
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ai4ce/SeeDo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/datasets/ai4ce/EgoPAT3Dv2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser. -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width teaser no-margin">
        
        <h2 class="title is-3">TLDR</h2>
        <p style="font-size: 20px; background-color: #8b00e13b;">
          Interpret human demonstration videos and generate robot action plans using a pipeline of keyframe selection, visual perception and vision language model reasoning.
        </p>
        <br>

        <center>
          <div id="overview">
            <img src="./static/images/teaser.jpg" style="width: 45vw; min-width: 330px;" alt="Teaser Image"> 
          </div>
        </center>
      </div>
    </div>
  </div>
</section>

<hr>


<!-- Video Comparison -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Video Comparison</h2>
        <div class="content has-text-justified">
          <center>
            <button class="btn" id="rgb_button" onclick="switchVideo('rgb')">RGB</button>
            <button class="btn" id="depth_button" onclick="switchVideo('depth')">Depth</button>
            <button class="btn" id="normal_button" onclick="switchVideo('normal')">Normal</button>
            <button class="btn" id="touch_button" onclick="switchVideo('touch')">Touch Integration</button>
            <br>
            <br>
            <video muted autoplay loop width="700px" height="400px" controls controls id="videoPlayer">
              <source src="./static/videos/rgb.mp4" type="video/mp4">
            </video>
          </center>
            <script>
                document.getElementById('rgb_button').classList.add('active');
                function switchVideo(videoSrc) {
                    document.getElementById('rgb_button').classList.remove('active');
                    document.getElementById('depth_button').classList.remove('active');
                    document.getElementById('normal_button').classList.remove('active');
                    document.getElementById('touch_button').classList.remove('active');

                    document.getElementById(videoSrc+'_button').classList.add('active');
                    const videoPlayer = document.getElementById('videoPlayer');
                    videoPlayer.src = './static/videos/' + videoSrc + '.mp4';
                    videoPlayer.play();  // Play automatically when the video is switched
                }
            </script>
        </div>
      </div>
    </div>
  </div>
</section> -->

<hr>

<!-- Method Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <center>
          <div id="overview">
            <img src="./static/images/main.jpg" style="width: 55vw; min-width: 330px;" alt="Method Image"> 
          </div>

          <button id="method_button1" class="btn" onclick="changeContent(1)">1. Keyframe Selection</button>
          <button id="method_button2" class="btn" onclick="changeContent(2)">2. Visual Perception</button>
          <button id="method_button3" class="btn" onclick="changeContent(3)">3. VLM Reasoning</button>

          <div class="content has-text-justified">
            <p id="method-text-content"> <b>Module 1: Keyframe Selection</b> 
              <br>We use APIs from the <a href="https://ai.google.dev/edge/mediapipe/solutions/guide">MediaPipe</a> to detect the hand keypoints and calculate the speed of the hand.
              <br>The speed plot is then interpolated to be continous and the valleys are used as the keyframe selections.
            </p>
            <!-- <center>
              <img id="image-content" src="./static/images/Method_1.png" style="width: 45vw; min-width: 330px;" />
            </center> -->
          </div>
        </center>

        <script>
          document.getElementById('method_button1').classList.add('active');
          function changeContent(buttonNumber) {
              const textElement = document.getElementById('method-text-content');
              const imageElement = document.getElementById('image-content');
              
              document.getElementById('method_button1').classList.remove('active');
              document.getElementById('method_button2').classList.remove('active');
              document.getElementById('method_button3').classList.remove('active');

              // Add 'active' class to the clicked button
              document.getElementById('method_button' + buttonNumber).classList.add('active');

              if (buttonNumber === 1) {
                  textElement.innerHTML = "<b>Module 1: Keyframe Selection</b> \
                                          <br>We use APIs from the <a href=\"https://ai.google.dev/edge/mediapipe/solutions/guide\">MediaPipe</a> to detect the hand keypoints and calculate the speed of the hand. \
                                          <br>The speed plot is then interpolated to be continous and the valleys are used as the keyframe selections. \
                                           ";
                  // imageElement.classList.remove('hidden');
                  // imageElement.src = "./static/images/Method_1.png";
                  // imageElement.alt = "Image 1";
              } else if (buttonNumber === 2) {
                  textElement.innerHTML = "<b>Module 2: Visual Perception</b> \
                                           <br><b>(1) Object recognition:</b>  \
                                           First we prompt the VLM to recognize objects in the initial frame and use the answer as the prompt to prompt <a href=\"https://github.com/IDEA-Research/GroundingDINO\">GroundingDINO</a>, an open vocabulary object detector to obtain the object boundin boxes in the initial frame.\
                                           <br>Then these bounding boxes are used to prompts for video tracking. \
                                           <br><b>(2) Video tracking and visual prompting:</b> \
                                           Leveraging the bounding boxes as input prompts, we use the <a href=\"https://github.com/facebookresearch/sam2\">Segment Anything 2 (SAM2)</a> to track objects throughout the video.\
                                           <br>The tracking results are annotated to the keyframes as visual prompts to aid the subsequent VLM reasoning module. \
                                           ";
                  // imageElement.classList.remove('hidden');
                  // imageElement.src = "./static/images/Method_2.png";
                  // imageElement.alt = "Image 2";
              } else if (buttonNumber === 3) {
                  textElement.innerHTML = "<b>Module 3: Vision Language Model (VLM) Reasoning</b> \
                                           <br><b>(1) Open vocabulary object recoginition</b>. The VLM is first used to recognize objects in the initial frame for the Visual Perception Module. \
                                           <br><b>(2) Reason action plans</b>. Then the VLM is used to parse every keyframe for action plans.\
                                           Specifically, we prompt the VLM to answer questions for each keyframe: what is in the image, whether the hand is operating an object, which is being picked and where is it dropping to.\
                                           The complete prompts can be found in our paper and the <a href=\"https://github.com/ai4ce/SeeDo\">codebase</a>.\
                                           <br>The textual robot plans are convereted to codes and executed on robots following the <a href=\"\">Code-as-Policies</a>. ";
                  // imageElement.classList.add('hidden');
                                           // imageElement.src = "image3.jpg";
                  // imageElement.alt = "Image 3";
              }
          }
        </script>

      </div>
    </div>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        
        <h2 class="title is-3">Data and demos</h2>
        <div class="content has-text-justified">
          <p>
            We collected a dataset of human demonstration videos in three diverse catogories: <b>vegetable organization</b>, <b>garment organization</b>, and <b>wooden block stacking</b>. 
          </p>
          <img src="./static/images/data.jpg" style="width: 55vw; min-width: 330px;" alt="Data Image">
          <p>
            Below are the data and corresponding results.
          </p>
        </div>
        <button id="demo_button1" class="btn" onclick="changeVideo(1)">Vegetable Organization</button>
        <button id="demo_button2" class="btn" onclick="changeVideo(2)">Garment Organization</button>
        <button id="demo_button3" class="btn" onclick="changeVideo(3)">Wooden Block Stacking</button>

        <div class="content has-text-justified"></div>
          <p id="demo-text-content"> <b>Human demonstration</b><br> 
            Here is a demonstration video for <b>vegetable organization</b>. The videos illustrate how a human arrange the vegetable toys into specific containers one by one.</p>
          <center>
            <video id="demo-video-content1" style="width: 45vw; min-width: 330px;" autoplay muted controls>
              <source src="./static/videos/vege-human-1.mp4" type="video/mp4">
            </video>
            <p id="demo-text-content-between1"> <b>Robot execution</b><br>
              In this video, the robot executes the vegetable organization task in the same order as the human demonstrates.</p>
            <video id="demo-video-content2" style="width: 45vw; min-width: 330px;" autoplay muted controls>
              <source src="./static/videos/vege-robot-1.mp4" type="video/mp4">
            </video>
            <!-- <p id="demo-text-content-between2"></p>
            <video id="demo-video-content3" style="width: 45vw; min-width: 330px;" autoplay muted controls>
              <source src="./static/videos/video3.mp4" type="video/mp4">
            </video> -->
          </center>
        </div>

        <script>
          document.getElementById('demo_button1').classList.add('active');
        
          function changeVideo(buttonNumber) {
            const textElement = document.getElementById('demo-text-content');
            const videoElement1 = document.getElementById('demo-video-content1');
            const videoElement2 = document.getElementById('demo-video-content2');
            // const videoElement3 = document.getElementById('video-content3');
            const textBetween1 = document.getElementById('demo-text-content-between1');
            // const textBetween2 = document.getElementById('demo-text-content-between2');
        
            document.getElementById('demo_button1').classList.remove('active');
            document.getElementById('demo_button2').classList.remove('active');
            document.getElementById('demo_button3').classList.remove('active');

            document.getElementById('demo_button' + buttonNumber).classList.add('active');
        
            if (buttonNumber === 1) {
              textElement.innerHTML = "<b>Human demonstration</b><br> Here is a demonstration video for <b>vegetable organization</b>. The videos illustrate how a human arrange the vegetable toys into specific containers one by one.";
              videoElement1.src = "./static/videos/vege-human-1.mp4";
              textBetween1.innerHTML = "<b>Robot execution</b><br> In this video, the robot executes the vegetable organization task in the same order as the human demonstrates.";
              videoElement2.src = "./static/videos/vege-robot-1.mp4";
              // textBetween2.innerHTML = "Here,";
              // videoElement3.src = "./static/videos/vegetable_video3.mp4";
        
            } else if (buttonNumber === 2) {
              textElement.innerHTML = "<b>Human demonstration</b><br> The <b>garment organization</b> demonstration videos display a human placing their garments into specific boxes.";
              videoElement1.src = "./static/videos/garment-human-1.mp4"; 
              textBetween1.innerHTML = "<b>Robot execution</b><br> In the robot execution the presented objects are different. The robot follows the same order as the human demonstration on the objects it recognizes.";
              videoElement2.src = "./static/videos/garment-robot-1.mp4"; 
              // textBetween2.innerHTML = "In this example, ";
              // videoElement3.src = "./static/videos/garment_video3.mp4";
        
            } else if (buttonNumber === 3) {
              textElement.innerHTML = "<b>Human demonstration</b><br> For <b>wooden block stacking</b>, the following video demonstrates how a human stacks four blocks.";
              videoElement1.src = "./static/videos/wooden-human-1.mp4";
              textBetween1.innerHTML = "<b>Robot execution</b><br> In the robot execution, the robot stacks the same structure following the same order.";
              videoElement2.src = "./static/videos/wooden-robot-1.mp4";
              // textBetween2.innerHTML = "In this video, ";
              // videoElement3.src = "./static/videos/block_video3.mp4";
            }
            videoElement1.load();
            videoElement2.load();
            videoElement3.load();
          }
        </script>
      </div>
    </div>
  </div>
</section>


<!-- BibTeX -->
<section class="section" id="BibTeX"> 
  <div class="container is-max-desktop content">
    <center>
    <h2 id="bibtexTitle" class="title">BibTeX</h2>
    <button id="copyButton" onclick="copyToClipboard()">
      <i class="fas fa-copy"></i>
    </button>
    <br>
    <pre style="display: inline-flex; text-align: left";><code id="bibtexInfo">
Coming Soon
      </code>
    </pre>
  </center>
  </div>
</section>
          
<!-- Acknowledgements   -->
<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    The work was supported in part through NSF grants 2238968, 2322242, and 2024882, and the NYU IT High Performance Computing resources, services, and staff expertise.
  </div>
</section>
          
<!-- Footer -->       
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
            This website is inspired by the project page of <a href="https://ai4ce.github.io/FusionSense/">FusionSense</a>.
          </p>
        </div>
      </div>
          </p>
        </div>
      </div>
  </div>
</footer>

</body>

</html>
